{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e66bc44-60ad-41be-ae65-cb6c8bc23021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef3f249-fa8b-4175-bc99-1c203b882b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize, Grayscale, Compose, ToTensor\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5203c7-4038-4c6c-96f1-872d0143b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b77bc-5bfa-4a85-8781-d2c4a04ebcb7",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13e8557-2d28-4739-b8a2-bfebb0e57a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ef5db-88fc-4761-bffd-2d6f9efb08c4",
   "metadata": {},
   "source": [
    "## Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b97046c8-27aa-4707-a1a8-8bbdf67c22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()\n",
    "\n",
    "import ale_py.roms as roms\n",
    "# roms.__all__ # https://www.gymlibrary.dev/environments/atari/complete_list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40206473-4f08-4bc3-8543-e847efa7a8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Game console created:\n",
      "  ROM file:  /home/muhamuttaqien/miniconda3/envs/deeprl/lib/python3.11/site-packages/AutoROM/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1718444681\n"
     ]
    }
   ],
   "source": [
    "from ale_py.roms import Breakout\n",
    "\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b8a9fc-ffc0-46dd-bc50-e7b076ff2825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\", render_mode='human')\n",
    "env.metadata['render_fps'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4741a13-9568-476d-9e2d-0cd73126b02f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, _ = env.reset()\n",
    "\n",
    "plt.imshow(observation)\n",
    "plt.title('Game State')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b501ad9-fff6-431a-8218-a19beafede7c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aae2ce-c648-4779-a6a8-13326a2c0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return frame / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b29654-8f8e-4b98-8f5c-64c54a638f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, frame, is_new_episode):\n",
    "    frame = preprocess_frame(frame)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((84, 84), dtype=np.float32) for i in range(4)], maxlen=4)\n",
    "        for _ in range(4):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "    stacked_state = np.stack(stacked_frames, axis=0)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8e38d-678a-467a-b29b-d9e83cb9f566",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de05e7c-3f26-4646-858e-0aad0b60aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(state), action, reward, np.stack(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b00ebd-4d71-4b6c-9099-05560df1e7c4",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d3b8a4b-2404-4f15-a0a4-1672e607a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQNCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        conv_output_size = self._get_conv_output(input_shape)\n",
    "        self.fc1 = nn.Linear(conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        \n",
    "        frame = torch.zeros(1, *shape)\n",
    "        frame = frame.permute(0, 3, 1, 2)\n",
    "\n",
    "        output = self.conv1(frame)\n",
    "        print(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7084b55-f2c2-4761-a4a9-2d3520ca6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, replay_buffer, gamma=0.99, epsilon_start=1.0, epsilon_end=0.02, epsilon_decay=1000000):\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_decay_factor = (epsilon_start - epsilon_end) / epsilon_decay\n",
    "        \n",
    "        self.model = DQNCNN(env.observation_space.shape, env.action_space.n)\n",
    "        self.target_model = DQNCNN(env.observation_space.shape, env.action_space.n)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.model(state).argmax().item()\n",
    "    \n",
    "    def train_step(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        done = torch.FloatTensor(done)\n",
    "        \n",
    "        q_values = self.model(state)\n",
    "        next_q_values = self.target_model(next_state)\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_value, expected_q_value)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon - self.epsilon_decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c86e569-77ac-4c24-9d3d-cbcebf2da4e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 210, 8, 8], expected input[1, 3, 210, 160] to have 210 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m replay_buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, replay_buffer)\n",
      "Cell \u001b[0;32mIn[36], line 11\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, env, replay_buffer, gamma, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay \u001b[38;5;241m=\u001b[39m epsilon_decay\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay_factor \u001b[38;5;241m=\u001b[39m (epsilon_start \u001b[38;5;241m-\u001b[39m epsilon_end) \u001b[38;5;241m/\u001b[39m epsilon_decay\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m DQNCNN(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m DQNCNN(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m, in \u001b[0;36mDQNCNN.__init__\u001b[0;34m(self, input_shape, num_actions)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m conv_output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_conv_output(input_shape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(conv_output_size, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m512\u001b[39m, num_actions)\n",
      "Cell \u001b[0;32mIn[35], line 18\u001b[0m, in \u001b[0;36mDQNCNN._get_conv_output\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m     15\u001b[0m frame \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(frame)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeprl/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeprl/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeprl/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeprl/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 210, 8, 8], expected input[1, 3, 210, 160] to have 210 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(10000)\n",
    "agent = DQNAgent(env, replay_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a149fc-990b-406e-9b0d-4bce5d6b443a",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4205c0a-ce7d-4019-8cf0-40cbbb34e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, num_episodes, batch_size, target_update):\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    stacked_frames = deque([np.zeros((84, 84), dtype=np.float32) for i in range(4)], maxlen=4)\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            agent.train_step(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "                all_rewards.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "                break\n",
    "        \n",
    "        if episode % target_update == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {np.mean(all_rewards[-10:])}\")\n",
    "    \n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c95615-7591-4e16-8791-32b69649391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "target_update = 10\n",
    "\n",
    "train(env, agent, num_episodes, batch_size, target_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa773d-5af6-49f2-b401-35f4036dd5f9",
   "metadata": {},
   "source": [
    "## Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1076ef9-a5da-41e1-a1a1-a5796290dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "num_episodes = 20\n",
    "max_steps_per_episode = 20\n",
    "rewards_per_episode = [] # List to store collected rewards\n",
    "\n",
    "while True:\n",
    "    for episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        \n",
    "        episode_reward = 0  # Total reward for the current episode\n",
    "    \n",
    "        for step in range(max_steps_per_episode):\n",
    "            env.render()\n",
    "    \n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "            episode_reward += reward  # Accumulate the reward for the episode\n",
    "    \n",
    "            if terminated or truncated:\n",
    "                break  # Exit the inner loop if the episode is terminated\n",
    "    \n",
    "        rewards_per_episode.append(episode_reward)  # Store the total reward for the episode\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db423f97-98ac-4311-b2df-d94959e75c72",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeprl)",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
